---
title: "Final_Project_Markdown"
author: "Ashton Chevallier, Lisa Minas, Olivier"
date: "August 12, 2017"
output:
  pdf_document: default
  html_document: default
---


#Dataload
```{r dataload, warning=FALSE, message=FALSE}
library(dplyr)
library(ggplot2)
library(stargazer)
library(lmtest)
#stargazer(both, )
#Change Directory
raw_data <- read.csv('C:/Users/achevall/Documents/UC Berkeley/w241/Final/BeatuyAd_CausalExperiment/Data/BeautyAds_July 19, 2017_22.18.csv')


```

#Exploratory Data Analysis

##Filters
```{r filter}
#First Filter out junk data
filter_data <- function(raw_data) {
  raw_data$RecordedDate <- as.POSIXct(strptime(as.character(raw_data$RecordedDate), '%Y-%m-%d %H:%M:%S'))
  data <- raw_data %>% 
    filter(Finished == 'True',
           Status == 'IP Address',
           Welcome == 'I agree', 
           Group != '',
           AudioCheck == 'Pineapple',
           RecordedDate > '2017-07-16 01:00:00')
}

data <- filter_data(raw_data)
```
We created an audio check to test whether people understood English and were paying attention to the survey. We are using the audiocheck to filter out non-compliers. The other filters are to remove junk data: we don't want partial responses, junk IPS, or people that didn't agree with our terms. The date filter is to ensure we are using the correct experiment timing.

##Summary Stats
```{r summary}
interesting_columns <- c('Race','Age','Gender','Location','Group')
summary(data[,interesting_columns])

percent_table <- function(column){
  print(head(sort(table(sort(column))/length(column), decreasing = TRUE)))
}

percent_table(data$Race)
```
We got excellent an split between Treatment and Control, which gives us confidence our randomization worked. The racial demographic split seems to be fairly representative of the US, but whites and asians seem to be  a bit over sampled. Our location is fairly well balanced but it seems like Texas is under-represented. Most importantly our gender split is relatively even. We'd prefer more females than males (as the US is slightly more female), especially because we think females will respond to treatment better. But overall, we are happy with the sampling and don't see any glaring bias.

##Exploratory Graphs
```{r age_graphs}

qplot(data$Age, geom = 'bar', fill = 'Count', main = 'Bar plot of Age')
```
Not a lot of old people on Mechanical Turk, this is expected because I doubt retirees are filling out surveys to pay the bills. But our sample is certainly biased towards the 'millenials'.
```{r}

qplot(data$Location, geom = 'bar', fill = 'r', main = 'Bar Plot of Location') + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  coord_flip()
```
Nothing too glaringly terrible, but Mechanical Turk seems much more popular in California than other places.
```{r}
#Rotate Race
qplot(data$Race, geom = 'bar', fill= 'Count', main = 'Bar Plot of Race') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  coord_flip()
```
Again, our distribution is fairly representative of the US. Nothing too biased. Although, we might not have enough power make any real causal claims on race.

##Variables of Interest
```{r}
personal_views <- c('Personal_Views_Confident',
                  'Personal_Views_Beautiful',
                  'Personal_Views_Beauty_Importance',
                  'Personal_Views_Relate_To_Model')

summary(data[,personal_views])
```

```{r}
print('Confidence')
percent_table(data$Personal_Views_Confident)
```
Our group seems to be a very confident bunch. With well over 2/3rds being confident, it seems unlikely we'll find any discerning difference between treatment and control.
```{r} 
print('Beauty')
percent_table(data$Personal_Views_Beautiful)
```
Considering how confident our Turk Users are, it's not surprising that they are also fairly positive in their views on their own beauty. There doesn't seem a lot of disagreement on their self beauty.
```{r}
print('Beauty Importance')
percent_table(data$Personal_Views_Beauty_Importance)
```
Similar to before, our Turks have a consistent view of beauty, with only about a 3rd disagreeing.
```{r}
print('Relate to Model')
percent_table(data$Personal_Views_Relate_To_Model)
```
Finally, the relating to the model seem to half a real split in the data. This is probably due to the treatment. 

We'll recode all of our responses to a logical numeric value. 1 will be agree, 2 for strongly agree, -1 for disagree, -2 for strongly disagree.
```{r clean}
recode <- function(field){
  out <- 0
  if(field == 'Agree'){
    out <- 1
  } else if(field == 'Strongly agree'){
    out <- 2
  } else if(field == 'Disagree'){
    out <- -1
  } else if(field == 'Strongly disagree'){
    out <- -2
  }
  out <- as.numeric(out)
  return(out)
}


data$Beautiful <- sapply(data$Personal_Views_Beautiful, FUN =  recode)
data$Confident <- sapply(data$Personal_Views_Confident, FUN =  recode)
data$Importance <- sapply(data$Personal_Views_Beauty_Importance, FUN =  recode)
data$Relate <- sapply(data$Personal_Views_Relate_To_Model, FUN =  recode)
```

```{r}
qplot(data = data, x = Confident, fill = Gender, main = 'Confidence Split by Gender' )

conf_analysis <- data %>% group_by(Gender,Confident) %>% summarize(cnt = n())
qplot(data = conf_analysis, x = Gender, y = Confident, size = cnt, color = cnt)
print(paste('Male Conf', mean(data$Confident[data$Gender=='Male'])))
print(paste('Female Conf', mean(data$Confident[data$Gender=='Female'])))
```

All of our turks a fairly confident, but the male population seems to be a little bit more confident than the ladies.

```{r, warning = FALSE}
qplot(data$Confident, data$Beautiful)
cor.test(data$Confident,data$Beautiful)
chisq.test(data$Confident, data$Beautiful)
```

#Randomization Validation

```{r random_validation}
qplot(data = data, x = Gender, geom = 'bar', fill = Group)
table(data$Gender)
table(data$Group)
```
It looks like our randomization worked very well. We got an even split on treatment and control and an almost even split on Male and Females. We have little reason to beleive the randomization didn't work properly.

#Randomization Inference

To check to see whether we just got lucky, or actually found a treatment effect before diving into linear models, we'll run a randomization inference check on the ATE between treatment and control.
```{r RI1, cache=TRUE}
treat <- filter(data, Group == 'Treatment')
control <- filter(data, Group == 'Control')

ate_Beautiful <- mean(treat$Beautiful) - mean(control$Beautiful)
ate_Confident <- mean(treat$Confident) - mean(control$Confident)
ate_Importance <- mean(treat$Importance) - mean(control$Importance)
ate_Relate <- mean(treat$Relate) - mean(control$Relate)

#
n <- 1e4
copy <- data
taus <- data.frame(matrix(NA, nrow = n, ncol = 4))
names(taus) <- c('Beauty','Confidence','Importance','Relate')
for(i in 1:n){
  copy$Group <- sample(data$Group)
  treat <- filter(copy, Group == 'Treatment')
  control <- filter(copy, Group == 'Control')
  
  taus[i,1] <- mean(treat$Beautiful) - mean(control$Beautiful)
  taus[i,2] <- mean(treat$Confident) - mean(control$Confident)
  taus[i,3] <- mean(treat$Importance) - mean(control$Importance)
  taus[i,4] <- mean(treat$Relate) - mean(control$Relate)
}

p_beauty <- sum(taus$Beauty > ate_Beautiful)/n
p_confident <- sum(taus$Confidence > ate_Confident)/n
p_relate <- sum(taus$Relate > ate_Relate)/n
p_importance <- sum(taus$Importance > ate_Importance)/n
print(paste('Pval Beauty',p_beauty))
print(paste('Pval Confident', p_confident))
print(paste('Pval Relate', p_relate))
print(paste('Pval Important', p_importance))
```

Our only variable that seems to have a real a statistically significant ATE is the relating to the model. As noted before, our group of Turks already had a a pretty high level of self beauty and confidence, so we didn't expect to see it change much with treatment.


##RI Plots
```{r rIplots}
qplot(taus$Beauty, bins = 30, main = paste('RI for Beauty Pvalue =', p_beauty)) + geom_vline(xintercept = ate_Beautiful)
qplot(taus$Confidence, bins = 30, main = paste('RI for Confidence Pvalue =',p_confident)) + geom_vline(xintercept = ate_Confident)
qplot(taus$Importance, bins = 30, main = paste('RI for Importance Pvalue =',p_importance)) + geom_vline(xintercept = ate_Importance)
qplot(taus$Relate, bins = 30, main = paste('RI for Relating Pvalue =',p_relate)) + geom_vline(xintercept = ate_Relate)

```

The distributions are fairly normal and centered at zero. We can be fairly confident in our treatment effect on Relating to the model, but not any of the other items.

#Linear Models

```{r lmbeauty}

m_beauty <- lm(data = data, Beautiful ~ Group)
summary(m_beauty)
```
The linear model version of statistical inference gives similar results to the RI, but with even larger pvalue. We're guessing the high levels of self beauty didn't leave much room for the treatment to have an effect.

```{r}
m_confident <- lm(data = data, Confident ~ Group)
summary(m_confident)
```
Similar to the beauty inference. It doesn't look like there was much room for a treatment to cause a noticable change.

```{r}
m_importance <- lm(data = data, Importance ~ Group)
summary(m_importance)
```
No surprises here. It looks like the video doesn't really change what you think about the importance of beauty.

```{r}
m_relate <- lm(data = data, Relate ~ Group)
summary(m_relate)
```
Our linear model confirms what we saw with the RI. Remember, we recoded Agree to 1 point and Strongly agree to 2 points, so the treatment effectively yields a .25 point increase in how much they person can relate to the model. It's not straightforward what the coefficient means, but effectively people relate more to the non-stereotypical model. We'll explore how the other co-variates effect the ATE.

```{r final}
stargazer(m_beauty, m_confident, m_importance, m_relate, header=F, type = 'text')
```
#Covariate Analysis
```{r}
m_beauty <- lm(data = data, Beautiful ~ Group + Gender)
summary(m_beauty)
data %>% group_by(Group, Gender) %>% summarize(avg = mean(Beautiful))
```
It looks like our treatment still isn't significant with the inclusion of gender as a co-variate, but interestingly enough women have a much higher rate of beauty than men do.


```{r}
m_confident <- lm(data = data, Confident ~ Group + Gender)
summary(m_confident)
data %>% group_by(Group, Gender) %>% summarise(avg = mean(Confident))
```
Again, adding the gender covariate didn't improve the model much.

```{r}
m_importance <- lm(data = data, Importance ~ Group + Gender)
summary(m_importance)
data %>% group_by(Group, Gender) %>% summarise(avg = mean(Importance))
```
Treatment effect remains insignificant with additional co-variates gender covariate. Somewhat surprisingly, men don't place as high importance on beauty.

```{r}
m_relate <- lm(data = data, Relate ~ Group + Gender)
summary(m_relate)
data %>% group_by(Group, Gender) %>% summarise(avg = mean(Relate))

```
With the gender co-variate our treatment remains statistically significiant. Unsurprisingly, the effect is a lot stronger for women than men.

#Covariate Summary
```{r}
stargazer(m_beauty, m_confident, m_importance, m_relate, header=F, type = 'text')
```
Gender did make a pretty big significance in almost every model, even when the treatment was insignificant. Since we only suspected the treatment effect to really have a effect on gender we aren't going fishing for other covariate significance.

# Image analysis
## Preparing the data 

Essentially we are counting a 1 if the response matched whether you were in treatment or control.
```{r cache=TRUE}
# Combine and recode randomization 1 & 2 for the images
images <- c('Passion', 'Coffee', 'Couple', 'Work', 'Fit')
questions <- c('_i_identify_', '_i_prefer_', '_o_prefer_', '_validate_')
randomization <- c('1', '2')

# Correct column names misspellings
data <- dplyr::rename(data, Coffee_validate_1 = Coffe_validate_1, 
            Fit_i_identify_1 = FIt_i_identify_1, 
            Work_i_identify_2 = work_i_identify_2)

# Recode image output to one column
for (image in images){
  for (question in questions){
    column1 <- paste(image, question, '1', sep = "")
    data[[column1]] <- ifelse(data[[column1]] == 'Ad 1', 2, ifelse(data[[column1]] == 'Ad 2', 1, 0))
    column2 <- paste(image, question, '2', sep = "")
    data[[column2]] <- ifelse(data[[column2]] == 'Ad 2', 2, ifelse(data[[column2]] == 'Ad 1', 1, 0))
    new_column <- paste(image, question, sep ="")
    data[[new_column]] <- data[[column1]] + data[[column2]] - 1
    data[[new_column]] <- ifelse(data[[new_column]] == -1, NA, data[[new_column]])
  }
}

```

## Getting the ATE for IMAGES questions
```{r}
images <- c('Passion', 'Coffee', 'Couple', 'Work', 'Fit')
questions <- c('_i_identify_', '_i_prefer_', '_o_prefer_', '_validate_')
models <- list(20)
i <- 1
for (image in images){
  for (question in questions){
    column <- paste(image, question, sep = "")
    l1 <- lm(data[[column]] ~ data[['Group']])
    models <- list(models, l1)
    i <- i + 1
  print(column)
  
  print(coeftest(l1))
  }
}
stargazer(models, type = 'text')
```
Only 1 question out of the 20 had any real significance. Essentially, for the passion photo, showing treatment led to preferring the non-stereotypical image. At an alpha rate of .05 we know that we could have simply gotten this result from a random selection at least 1 out of 20 times. To make sure we there is a real effect here, we'll need to run the experiment again, otherwise we might be fishing for significance. 

## Taking the total and average by questions for all images
```{r}
questions <- c('_i_identify_', '_i_prefer_', '_o_prefer_', '_validate_')
images <- c('Passion', 'Coffee', 'Couple', 'Work', 'Fit')

for (question in questions){
  question_average <- paste(question, 'average', sep = "")
  data[[question_average]] <- rep(0, nrow(data))
  for (image in images){
    column <- paste(image, question, sep = "")
    data[[question_average]] <- data[[question_average]] + data[[column]]
  }
}



average_lm <- function(data, question){
  question_average <- paste(question, 'average', sep = "")
  #print(question_average)
  question_average <- lm(data[[question_average]] ~ data[['Group']] + data$Gender)
  #print(coeftest(question_average))
  return(question_average)
}

lm_i_identify <- average_lm(data, '_i_identify_')
lm_i_prefer <- average_lm(data, '_i_prefer_')
lm_o_prefer <- average_lm(data, '_o_prefer_')
lm_validate <- average_lm(data, '_validate_')

stargazer(lm_i_identify, lm_i_prefer, lm_o_prefer, lm_validate, header=T, type = 'text',
          column.labels = c('I Identify','I Prefer','Others Prefer','Validate'))
```
Because the original data set was essentially 20 interactions between images and questions, we wanted to summarize the effects to each question. In aggregate, the measurement still makes sense. We are in effect seeing if each survey taker matches up to the stereotypical or non-stereotypical image and seeing if the treatment makes any difference. Even in aggregate none of the questions had any statistically significant treatment effect. Although, interestingly males seemed to match up with their photos of their treatment more than females.


#Second data set 
## Load, Clean & Recode the data 
```{r}
raw_data2 <- read.csv('C:/Users/achevall/Documents/UC Berkeley/w241/Final/BeatuyAd_CausalExperiment/Data/BeautyAds_August 12, 2017_21.50.csv')

data2 <- filter_data(raw_data2)
data2 <- filter(data2, RecordedDate > '2017-08-10 00:00:00')

data2$Beautiful <- sapply(data2$Personal_Views_Beautiful, FUN =  recode)
data2$Confident <- sapply(data2$Personal_Views_Confident, FUN =  recode)
data2$Importance <- sapply(data2$Personal_Views_Beauty_Importance, FUN =  recode)
data2$Relate <- sapply(data2$Personal_Views_Relate_To_Model, FUN =  recode)

# Combine and recode randomization 1 & 2 for the images
images <- c('Passion', 'Coffee', 'Couple', 'Work', 'Fit')
questions <- c('_i_identify_', '_i_prefer_', '_o_prefer_', '_validate_')
randomization <- c('1', '2')

# Correct column names misspellings
data2 <- dplyr::rename(data2, Coffee_validate_1 = Coffe_validate_1, 
            Fit_i_identify_1 = FIt_i_identify_1, 
            Work_i_identify_2 = work_i_identify_2)

# Recode image output to one column
for (image in images){
  for (question in questions){
    column1 <- paste(image, question, '1', sep = "")
    data2[[column1]] <- ifelse(data2[[column1]] == 'Ad 1', 2, ifelse(data2[[column1]] == 'Ad 2', 1, 0))
    column2 <- paste(image, question, '2', sep = "")
    data2[[column2]] <- ifelse(data2[[column2]] == 'Ad 2', 2, ifelse(data2[[column2]] == 'Ad 1', 1, 0))
    new_column <- paste(image, question, sep ="")
    data2[[new_column]] <- data2[[column1]] + data2[[column2]] - 1
    data2[[new_column]] <- ifelse(data2[[new_column]] == -1, NA, data2[[new_column]])
  }
}

```
## Text analysis
```{r}
columns_to_analyse <- c('Beautiful', 'Confident', 'Importance', 'Relate')
models <- list(4)
for (column in columns_to_analyse){
  l1 <- lm(data2[[column]] ~ data2[['Group']] + data2[['Gender']])
 
  models <- list(models, l1)
}
stargazer(models, header = FALSE, type = 'text', column.labels = columns_to_analyse)
```
Our second experiment confirms the treatment effect on relating to the models in the non-stereotypical ad.

## Image analysis

```{r}
images <- c('Passion', 'Coffee', 'Couple', 'Work', 'Fit')
questions <- c('_i_identify_', '_i_prefer_', '_o_prefer_', '_validate_')

for (image in images){
  for (question in questions){
    column <- paste(image, question, sep = "")
    l1 <- lm(data2[[column]] ~ data2[['Group']])
  }
}
```

## Taking the total and average by questions for all images
```{r}
questions <- c('_i_identify_', '_i_prefer_', '_o_prefer_', '_validate_')
images <- c('Passion', 'Coffee', 'Couple', 'Work', 'Fit')

for (question in questions){
  question_average <- paste(question, 'average', sep = "")
  data2[[question_average]] <- rep(0, nrow(data2))
  for (image in images){
    column <- paste(image, question, sep = "")
    data2[[question_average]] <- data2[[question_average]] + data2[[column]]
  }
}

# for (question in questions){
#   question_average <- paste(question, 'average', sep = "")
#   print(question_average)
#   print(summary(data2[[question_average]]))
#   l1 <- lm(data2[[question_average]] ~ data2[['Group']] + data2$Gender)
#   print(summary(l1))
# }

lm_i_identify_ <- average_lm(data2, '_i_identify_')
lm_i_prefer_ <- average_lm(data2, '_i_prefer_')
lm_o_prefer_ <- average_lm(data2, '_o_prefer_')
lm_validate_ <- average_lm(data2, '_validate_')

stargazer(lm_i_identify_, lm_i_prefer_, lm_o_prefer_, lm_validate_, header=T, type = 'text',
          column.labels = c('I Identify','I Prefer','Others Prefer','Validate'))
```
Interestingly, our 2nd experiment did find significance of people matching the image that they thought others would prefer. It's hard to trust this result, as the 1st experiment performed so differently.

